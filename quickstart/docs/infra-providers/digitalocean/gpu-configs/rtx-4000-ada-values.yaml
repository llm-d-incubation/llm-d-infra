# GPU Memory Optimized Configuration for NVIDIA RTX 4000 Ada (20GB VRAM)
# Optimized for DigitalOcean Kubernetes clusters
# Memory allocation: ~17GB available (85% of 20GB)

sampleApplication:
  # Resource requests and limits for RTX 4000 Ada
  resources:
    limits:
      nvidia.com/gpu: "1"
      memory: "32Gi" # Host memory for RTX 4000 Ada nodes
      cpu: "8"
    requests:
      nvidia.com/gpu: "1"
      memory: "16Gi"
      cpu: "4"
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

  # Decode configuration with memory optimizations
  decode:
    extraArgs:
      - "--max-model-len"
      - "8192" # Conservative sequence length for 20GB VRAM
      - "--gpu-memory-utilization"
      - "0.85" # Use 85% of GPU memory
      - "--enforce-eager" # Disable CUDA graph to save memory
      - "--tensor-parallel-size"
      - "1" # Single GPU setup
      - "--block-size"
      - "16" # Smaller block size for better memory efficiency

  # Prefill configuration with memory optimizations
  prefill:
    extraArgs:
      - "--max-model-len"
      - "8192"
      - "--gpu-memory-utilization"
      - "0.85"
      - "--enforce-eager"
      - "--tensor-parallel-size"
      - "1"
      - "--block-size"
      - "16"

# Enable comprehensive metrics collection
modelservice:
  metrics:
    enabled: true
    serviceMonitor:
      interval: 15s
  epp:
    metrics:
      enabled: true
      serviceMonitor:
        interval: 10s
  vllm:
    metrics:
      enabled: true
