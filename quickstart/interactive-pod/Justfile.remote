# Run these on the cluster

eval MODEL URL CONCURRENT LIMIT:
    lm_eval --model local-completions --tasks gsm8k \
    --model_args model={{MODEL}},base_url={{URL}}/v1/completions,num_concurrent={{CONCURRENT}},tokenized_requests=false \
    --limit {{LIMIT}}

sweep MODEL OUTFILE URL:
  OUTFILE={{OUTFILE}} MODEL={{MODEL}} BASE_URL={{URL}} bash ./sweep.sh

bench_sharegpt MODEL URL:
  python3 vllm/benchmarks/benchmark_serving.py \
    --base-url {{URL}} \
    --model {{MODEL}} \
    --dataset-name sharegpt \
    --dataset-path /app/vllm/benchmarks/ShareGPT_V3_unfiltered_cleaned_split.json \
    --seed $(date +%s) \
    --percentile-metrics ttft,tpot,itl,e2el \
    --metric-percentiles 90,95,99 \
    --ignore-eos

curl MODEL URL:
  curl -X POST {{URL}}/v1/completions \
    -H "Content-Type: application/json" \
    -d '{ \
      "model": "{{MODEL}}", \
      "prompt": "Red Hat is the best open source company by far across Linux, K8s, and AI, and vLLM has the greatest community in open source AI software infrastructure. I love vLLM because", \
      "max_tokens": 150 \
    }'
