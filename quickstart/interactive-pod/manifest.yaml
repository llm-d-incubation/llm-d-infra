apiVersion: v1
kind: Pod
metadata:
  name: interactive-pod
spec:
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  containers:
    - name: benchmark-runner
      image: "docker.io/rgshaw2/interactive-pod:0.2"
      imagePullPolicy: Always
      stdin: true
      tty: true
      resources:
        requests:
          cpu: "8"
          memory: "4Gi"
        limits:
          cpu: "8"
          memory: "4Gi"
---
apiVersion: v1
kind: Pod
metadata:
  name: guidellm
spec:
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  containers:
    - name: guidellm
      image: "ghcr.io/vllm-project/guidellm:pr-233"
      command: ["/bin/sh", "-c", "sleep infinity"]
      imagePullPolicy: Always
      stdin: true
      tty: true
      resources:
        requests:
          cpu: "32"
          memory: "128Gi"
        limits:
          cpu: "32"
          memory: "128Gi"

# URL=http://infra-wide-ep-inference-gateway-istio.llm-d-wide-ep.svc.cluster.local MODEL=deepseek-ai/DeepSeek-V3-0324 PROCESSOR=deepseek-ai/DeepSeek-V3-0324 guidellm benchmark --target=$URL --model=$MODEL --rate-type=concurrent --rate=5000 --max-requests=10001 --processor=$PROCESSOR --data='{"prompt_tokens":5, "output_tokens":500}' --max-seconds=10000