multinode: true

modelArtifacts:
  uri: "hf://deepseek-ai/DeepSeek-R1-0528"
  size: 100Gi
  authSecretName: "llm-d-hf-token"

routing:
  modelName: deepseek-ai/DeepSeek-R1-0528
  servicePort: 8000
  parentRefs:
    - group: gateway.networking.k8s.io
      kind: Gateway
      name: infra-wide-ep-inference-gateway

  proxy:
    image: "ghcr.io/llm-d/llm-d-routing-sidecar:v0.2.0"
    secure: false
    debugLevel: 1
    connector: nixlv2

  inferenceModel:
    criticality: Critical
    create: true

  inferencePool:
    create: false
    name: gaie-wide-ep

  httpRoute:
    create: true

  epp:
    create: false

decode:
  create: true
  replicas: 1
  acceleratorTypes:
    labelKey: gpu.nvidia.com/model
    labelValues:
      - H200
  parallelism:
    # TODO: The value for parallelism.data is a hack to get the pod-per-node case working.
    # This must equal the number of nodes rather than the dp_size.
    data: 4
    tensor: 1 # these will be derived based performance testing
  monitoring:
    podmonitor:
      enabled: false
      portName: "metrics" # decode vLLM service port (from routing.proxy.targetPort)
      path: "/metrics"
      interval: "30s"
  containers:
    - name: vllm-worker-decode
      image: "rgshaw2/vllm-torch-28-0.2"
      imagePullPolicy: Always
      workingDir: /code
      command: ["/bin/bash", "-c"]
      priorityClassName: "priorityclass.scheduling.k8s.io/moe-testing"
      modelCommand: "custom"
      args:
        - |-
          #################
          # RUN vLLM decode worker
          #################
          START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))

          source /opt/vllm/bin/activate
          exec vllm serve \
            deepseek-ai/DeepSeek-R1-0528 \
            --port 8200 \
            --disable-log-requests \
            --disable-uvicorn-access-log \
            --enable-expert-parallel \
            --data-parallel-hybrid-lb \
            --tensor-parallel-size $TP_SIZE \
            --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
            --data-parallel-size-local $DP_SIZE_LOCAL \
            --data-parallel-address ${LWS_LEADER_ADDRESS} \
            --data-parallel-rpc-port 5555 \
            --data-parallel-start-rank $START_RANK \
            --trust-remote-code \
            --enable-eplb \
            --num-redundant-experts 32 \
            --eplb-window-size 1000 \
            --eplb-step-interval 3000 \
            --eplb-log-balancedness

      ports:
        - containerPort: 8200
          name: metrics
          protocol: TCP
      env:
        - name: VLLM_MOE_DP_CHUNK_SIZE
          value: "512"
        - name: DP_SIZE_LOCAL
          value: "8"
        - name: TRITON_LIBCUDA_PATH
          value: "/usr/lib64"
        - name: HF_HUB_DISABLE_XET
          value: "1"
        - name: VLLM_SKIP_P2P_CHECK
          value: "1"
        - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
          value: "1"
        - name: VLLM_USE_DEEP_GEMM
          value: "1"
        - name: VLLM_ALL2ALL_BACKEND
          value: "deepep_low_latency"
        - name: NVIDIA_GDRCOPY
          value: "enabled"
        - name: NVSHMEM_DEBUG
          value: "INFO"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibgda"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: VLLM_LOGGING_LEVEL
          value: "INFO"
        - name: HF_HUB_CACHE
          value: /huggingface-cache
        - name: VLLM_TORCH_PROFILER_DIR
          value: /home/vllm
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
      resources:
        limits:
          memory: 512Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: "8"
          rdma/ib: 1
        requests:
          cpu: 32
          memory: 512Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: "8"
          rdma/ib: 1
      mountModelVolume: false
      volumeMounts:
        - name: dshm
          mountPath: /dev/shm
        - name: hf-cache
          mountPath: /huggingface-cache

  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 8Gi
    - name: hf-cache
      hostPath:
        path: /mnt/local/hf-cache
        type: DirectoryOrCreate

prefill: 
  replicas: 0
