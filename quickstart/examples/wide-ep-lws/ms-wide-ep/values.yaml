multinode: true

modelArtifacts:
  uri: "hf://Qwen/Qwen3-30B-A3B-FP8"
  size: 100Gi
  authSecretName: "llm-d-hf-token"

routing:
  modelName: Qwen/Qwen3-30B-A3B-FP8
  servicePort: 8000
  parentRefs:
    - group: gateway.networking.k8s.io
      kind: Gateway
      name: infra-wide-ep-inference-gateway

  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.2.0-RC1
    connector: nixlv2
    secure: false

  inferenceModel:
    create: false

  inferencePool:
    create: false
    name: gaie-wide-ep

  httpRoute:
    create: true

  epp:
    create: false

decode:
  subGroupPolicy:
    subGroupSize: 8
  create: true
  replicas: 1
  acceleratorTypes:
    labelKey: gpu.nvidia.com/model
    labelValues:
      - H200
  parallelism:
    data: 8 # these will be derived based performance testing
    tensor: 1 # these will be derived based performance testing
  containers:
    - name: vllm-worker-decode
      image: "quay.io/tms/llm-d-cks:0.2.0-beta0"
      imagePullPolicy: Always
      workingDir: /code
      command: ["/bin/bash","-c"]
      modelCommand: "custom"
      args:
        - |
          ###################################################
          # Figure out which GPU to use when 'privileged: true'.
          # NVIDIA_VISIBLE_DEVICES tells us which GPU UUIDs to use, but
          # vLLM requires that CPU_VISIBLE_DEVICES is set to device ordinals.
          # This remaps the GPU UUIDs to indices.

          # --- 1. Collect the UUIDs that the container has access to -------------------
          mapfile -t ALLOCATED_GPU_UUIDS < <(ls -1 "$NVIDIA_VISIBLE_DEVICES" | sort)

          echo "Allocated GPU UUIDs (${#ALLOCATED_GPU_UUIDS[@]} found):"
          printf '  • %s\n' "${ALLOCATED_GPU_UUIDS[@]}"

          # --- 2. Build a map of UUID → index from nvidia‑smi --------------------------
          GPU_MAP_DATA="$(nvidia-smi --query-gpu=uuid,index --format=csv,noheader)"
          echo "nvidia-smi uuid,index pairs:"
          echo "$GPU_MAP_DATA"

          declare -a ALLOCATED_GPU_INDICES=()

          for uuid in "${ALLOCATED_GPU_UUIDS[@]}"; do
            idx="$(
              printf '%s\n' "$GPU_MAP_DATA" |
              awk -F',' -v target_uuid="$uuid" '
                {
                  gsub(/^[ \t]+|[ \t]+$/, "", $1)   # trim spaces in $1 (uuid)
                  if ($1 == target_uuid) {
                    gsub(/^[ \t]+|[ \t]+$/, "", $2) # trim spaces in $2 (index)
                    print $2
                    exit
                  }
                }'
            )"
            if [ -n "$idx" ]; then
              ALLOCATED_GPU_INDICES+=("$idx")
            else
              echo "⚠️  Warning: UUID $uuid not found in nvidia‑smi output" >&2
            fi
          done


          echo "Using GPU Device ID: $ALLOCATED_GPU_INDICES"
          #################
          # RUN vLLM
          #################
          export CUDA_VISIBLE_DEVICES=${ALLOCATED_GPU_INDICES}

          # vllm serve \
          source /opt/vllm/bin/activate
          exec vllm serve \
            Qwen/Qwen3-30B-A3B-FP8 \
            --port 8200 \
            --disable-log-requests \
            --enable-expert-parallel \
            --tensor-parallel-size $TP_SIZE \
            --data-parallel-size $(LWS_GROUP_SIZE) \
            --data-parallel-address $(LWS_LEADER_ADDRESS) \
            --data-parallel-rpc-port 5555 \
            --data-parallel-rank $(LWS_WORKER_INDEX) \
            --trust-remote-code \
            --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
      env:
        - name: NCCL_DEBUG
          value: INFO
        - name: UCX_TLS
          value: "all"
        - name: UCX_NET_DEVICES
          value: "^posix,^cma"
        - name: TRITON_LIBCUDA_PATH
          value: "/usr/lib64"
        - name: HF_HUB_DISABLE_XET
          value: "1"
        - name: VLLM_SKIP_P2P_CHECK
          value: "1"
        - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
          value: "1"
        # - name: TP_SIZE
        #   value: "1"
        - name: VLLM_USE_DEEP_GEMM
          value: "1"
        - name: VLLM_ALL2ALL_BACKEND
          value: "deepep_low_latency" # TEMP workaround to avoid issues with nvshmem
        - name: NVIDIA_GDRCOPY
          value: "enabled"
        - name: NVSHMEM_DEBUG
          value: "INFO"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibgda"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: VLLM_LOGGING_LEVEL
          value: "DEBUG"
        - name: HF_HUB_CACHE
          value: /huggingface-cache
        # - name: HF_TOKEN
        #   valueFrom:
        #     secretKeyRef:
        #       name: llm-d-hf-token
        #       key: HF_TOKEN
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
      securityContext:
        privileged: true
        capabilities:
          add:
          - "IPC_LOCK"
          - "SYS_RAWIO"
      resources:
        limits:
          memory: 128Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: "1"
          rdma/ib: 1
        requests:
          cpu: 8
          memory: 128Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: "1"
          rdma/ib: 1
      mountModelVolume: false

prefill:
  subGroupPolicy:
    subGroupSize: 8
  create: true
  replicas: 1
  acceleratorTypes:
    labelKey: gpu.nvidia.com/model
    labelValues:
      - H200
  parallelism:
    data: 8 # these will be derived based performance testing
    tensor: 1 # these will be derived based performance testing
  containers:
    - name: vllm-worker-prefill
      image: "quay.io/tms/llm-d-cks:0.2.0-beta0"
      imagePullPolicy: Always
      workingDir: /code
      command: ["/bin/bash","-c"]
      modelCommand: "custom"
      args:
        - |
          ###################################################
          # Figure out which GPU to use when 'privileged: true'.
          # NVIDIA_VISIBLE_DEVICES tells us which GPU UUIDs to use, but
          # vLLM requires that CPU_VISIBLE_DEVICES is set to device ordinals.
          # This remaps the GPU UUIDs to indices.

          # --- 1. Collect the UUIDs that the container has access to -------------------
          mapfile -t ALLOCATED_GPU_UUIDS < <(ls -1 "$NVIDIA_VISIBLE_DEVICES" | sort)

          echo "Allocated GPU UUIDs (${#ALLOCATED_GPU_UUIDS[@]} found):"
          printf '  • %s\n' "${ALLOCATED_GPU_UUIDS[@]}"

          # --- 2. Build a map of UUID → index from nvidia‑smi --------------------------
          GPU_MAP_DATA="$(nvidia-smi --query-gpu=uuid,index --format=csv,noheader)"
          echo "nvidia-smi uuid,index pairs:"
          echo "$GPU_MAP_DATA"

          declare -a ALLOCATED_GPU_INDICES=()

          for uuid in "${ALLOCATED_GPU_UUIDS[@]}"; do
            idx="$(
              printf '%s\n' "$GPU_MAP_DATA" |
              awk -F',' -v target_uuid="$uuid" '
                {
                  gsub(/^[ \t]+|[ \t]+$/, "", $1)   # trim spaces in $1 (uuid)
                  if ($1 == target_uuid) {
                    gsub(/^[ \t]+|[ \t]+$/, "", $2) # trim spaces in $2 (index)
                    print $2
                    exit
                  }
                }'
            )"
            if [ -n "$idx" ]; then
              ALLOCATED_GPU_INDICES+=("$idx")
            else
              echo "⚠️  Warning: UUID $uuid not found in nvidia‑smi output" >&2
            fi
          done


          echo "Using GPU Device ID: $ALLOCATED_GPU_INDICES"
          #################
          # RUN vLLM
          #################
          export CUDA_VISIBLE_DEVICES=${ALLOCATED_GPU_INDICES}

          # vllm serve \
          source /opt/vllm/bin/activate
          exec vllm serve \
            Qwen/Qwen3-30B-A3B-FP8 \
            --port 8200 \
            --disable-log-requests \
            --enable-expert-parallel \
            --tensor-parallel-size $TP_SIZE \
            --data-parallel-size ${LWS_GROUP_SIZE} \
            --data-parallel-address ${LWS_LEADER_ADDRESS} \
            --data-parallel-rpc-port 5555 \
            --data-parallel-rank ${LWS_WORKER_INDEX} \
            --trust-remote-code \
            --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
      env:
        - name: NCCL_DEBUG
          value: INFO
        - name: UCX_TLS
          value: "all"
        - name: UCX_NET_DEVICES
          value: "^posix,^cma"
        - name: TRITON_LIBCUDA_PATH
          value: "/usr/lib64"
        - name: HF_HUB_DISABLE_XET
          value: "1"
        - name: VLLM_SKIP_P2P_CHECK
          value: "1"
        - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
          value: "1"
        # - name: TP_SIZE
        #   value: "1"
        - name: VLLM_USE_DEEP_GEMM
          value: "1"
        - name: VLLM_ALL2ALL_BACKEND
          value: "deepep_high_throughput" # TEMP workaround to avoid issues with nvshmem
        - name: NVIDIA_GDRCOPY
          value: "enabled"
        # - name: NVSHMEM_DEBUG
        #   value: "INFO"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibgda"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: VLLM_LOGGING_LEVEL
          value: "DEBUG"
        - name: HF_HUB_CACHE
          value: /huggingface-cache
        # - name: HF_TOKEN
        #   valueFrom:
        #     secretKeyRef:
        #       name: llm-d-hf-token
        #       key: HF_TOKEN
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
      securityContext:
        privileged: true
        capabilities:
          add:
          - "IPC_LOCK"
          - "SYS_RAWIO"
      resources:
        limits:
          memory: 128Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: "1"
          rdma/ib: 1
        requests:
          cpu: 8
          memory: 128Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: "1"
          rdma/ib: 1
      mountModelVolume: false
