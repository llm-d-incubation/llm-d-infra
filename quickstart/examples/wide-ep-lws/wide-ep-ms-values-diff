diff --git a/quickstart/examples/wide-ep-lws/ms-wide-ep/values.yaml b/quickstart/examples/wide-ep-lws/ms-wide-ep/values.yaml
index 60337c7..0c00df1 100644
--- a/quickstart/examples/wide-ep-lws/ms-wide-ep/values.yaml
+++ b/quickstart/examples/wide-ep-lws/ms-wide-ep/values.yaml
@@ -1,12 +1,13 @@
 multinode: true

 modelArtifacts:
-  uri: "hf://deepseek-ai/DeepSeek-R1-0528"
-  size: 100Gi
+  uri: "hf://Qwen/Qwen3-235B-A22B-Thinking-2507-FP8"
+  size: 240Gi
   authSecretName: "llm-d-hf-token"
+  # name: "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8"

 routing:
-  modelName: deepseek-ai/DeepSeek-R1-0528
+  modelName: Qwen/Qwen3-235B-A22B-Thinking-2507-FP8
   servicePort: 8000
   parentRefs:
     - group: gateway.networking.k8s.io
@@ -87,7 +88,7 @@ decode:
       interval: "30s"
   containers:
     - name: vllm-worker-decode
-      image: "ghcr.io/llm-d/llm-d:v0.2.0"
+      image: "ghcr.io/llm-d/llm-d-dev@sha256:dcb6b80a53d058e62dcbfc1166bf9e78419a62ea1e424489c85bc872f229a8e7"
       imagePullPolicy: Always
       workingDir: /code
       command: ["/bin/bash", "-c"]
@@ -101,7 +102,7 @@ decode:

           source /opt/vllm/bin/activate
           exec vllm serve \
-            deepseek-ai/DeepSeek-R1-0528 \
+            Qwen/Qwen3-235B-A22B-Thinking-2507-FP8 \
             --port 8200 \
             --disable-log-requests \
             --disable-uvicorn-access-log \
@@ -141,7 +142,7 @@ decode:
         - name: VLLM_USE_DEEP_GEMM
           value: "1"
         - name: VLLM_ALL2ALL_BACKEND
-          value: "deepep_low_latency"
+          value: "pplx"
         - name: NVIDIA_GDRCOPY
           value: "enabled"
         - name: NVSHMEM_DEBUG
@@ -161,7 +162,7 @@ decode:
         - name: VLLM_LOGGING_LEVEL
           value: "INFO"
         - name: HF_HUB_CACHE
-          value: /huggingface-cache
+          value: /tmp/mnt/models
         - name: VLLM_NIXL_SIDE_CHANNEL_HOST
           valueFrom:
             fieldRef:
@@ -190,17 +191,15 @@ decode:
         - name: dshm
           mountPath: /dev/shm
         - name: hf-cache
-          mountPath: /huggingface-cache
-
+          mountPath: /tmp/mnt
   volumes:
     - name: dshm
       emptyDir:
         medium: Memory
         sizeLimit: 1Gi
     - name: hf-cache
-      hostPath:
-        path: /mnt/local/hf-cache
-        type: DirectoryOrCreate
+      persistentVolumeClaim:
+        claimName: hf-model-pvc

 prefill:
   create: true
@@ -222,7 +221,7 @@ prefill:
       interval: "30s"
   containers:
     - name: vllm-worker-prefill
-      image: "ghcr.io/llm-d/llm-d:v0.2.0"
+      image: "ghcr.io/llm-d/llm-d-dev@sha256:dcb6b80a53d058e62dcbfc1166bf9e78419a62ea1e424489c85bc872f229a8e7"
       imagePullPolicy: Always
       workingDir: /code
       command: ["/bin/bash", "-c"]
@@ -236,7 +235,7 @@ prefill:

           source /opt/vllm/bin/activate
           exec vllm serve \
-            deepseek-ai/DeepSeek-R1-0528 \
+            Qwen/Qwen3-235B-A22B-Thinking-2507-FP8 \
             --port 8000 \
             --disable-log-requests \
             --disable-uvicorn-access-log \
@@ -275,7 +274,7 @@ prefill:
         - name: VLLM_USE_DEEP_GEMM
           value: "1"
         - name: VLLM_ALL2ALL_BACKEND
-          value: "deepep_high_throughput"
+          value: "pplx"
         - name: NVIDIA_GDRCOPY
           value: "enabled"
         - name: NVSHMEM_DEBUG
@@ -295,7 +294,7 @@ prefill:
         - name: VLLM_LOGGING_LEVEL
           value: "INFO"
         - name: HF_HUB_CACHE
-          value: /huggingface-cache
+          value: /tmp/mnt/models
         - name: VLLM_NIXL_SIDE_CHANNEL_HOST
           valueFrom:
             fieldRef:
@@ -324,14 +323,12 @@ prefill:
         - name: dshm
           mountPath: /dev/shm
         - name: hf-cache
-          mountPath: /huggingface-cache
-
+          mountPath: /tmp/mnt
   volumes:
     - name: dshm
       emptyDir:
         medium: Memory
         sizeLimit: 1Gi
     - name: hf-cache
-      hostPath:
-        path: /mnt/local/hf-cache
-        type: DirectoryOrCreate
+      persistentVolumeClaim:
+        claimName: hf-model-pvc
