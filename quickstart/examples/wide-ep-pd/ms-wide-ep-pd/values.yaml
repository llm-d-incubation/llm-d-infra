multinode: true

modelArtifacts:
  uri: "hf://deepseek-ai/DeepSeek-V3-0324"
  size: 100Gi
  authSecretName: "llm-d-hf-token"

routing:
  modelName: deepseek-ai/DeepSeek-V3-0324
  servicePort: 8000
  parentRefs:
    - group: gateway.networking.k8s.io
      kind: Gateway
      name: infra-wide-ep-pd-inference-gateway

  proxy:
    image: "ghcr.io/llm-d/llm-d-routing-sidecar:v0.2.0"
    secure: false
    debugLevel: 1
    connector: nixlv2

  inferenceModel:
    criticality: Critical
    create: true

  inferencePool:
    create: true
    name: ms-wide-ep-pd

  httpRoute:
    create: true

  epp:
    create: true
    image: ghcr.io/llm-d/llm-d-inference-scheduler:v0.2.1
    debugLevel: 1
    pluginsConfigFile: "pd-config.yaml"
    pluginsCustomConfig:
      pd-config.yaml: |
        # ALWAYS DO PD IN THIS EXAMPLE (THRESHOLD 0)
        apiVersion: inference.networking.x-k8s.io/v1alpha1
        kind: EndpointPickerConfig
        plugins:
        - type: prefill-header-handler
        - type: prefill-filter
        - type: decode-filter
        - type: random-picker
          parameters:
            maxNumOfEndpoints: 1
        - type: pd-profile-handler
          parameters:
            threshold: 0
            hashBlockSize: 5
        schedulingProfiles:
        - name: prefill
          plugins:
          - pluginRef: random-picker
        - name: decode
          plugins:
          - pluginRef: random-picker

decode:
  priorityClassName: moe-testing
  create: true
  replicas: 1
  parallelism:
    # TODO: The value for parallelism.data is a hack to get the pod-per-node case working.
    # This must equal the number of nodes rather than the dp_size.
    data: 9
    tensor: 1 # these will be derived based performance testing
  monitoring:
    podmonitor:
      enabled: false
      portName: "metrics" # decode vLLM service port (from routing.proxy.targetPort)
      path: "/metrics"
      interval: "30s"
  containers:
    - name: vllm-worker-decode
      image: "rgshaw2/vllm-torch-28-0.2"
      imagePullPolicy: Always
      workingDir: /code
      command: ["/bin/bash", "-c"]
      modelCommand: "custom"
      args:
        - |-
          #################
          # RUN vLLM decode worker
          #################
          START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))

          source /opt/vllm/bin/activate
          exec vllm serve \
            deepseek-ai/DeepSeek-V3-0324 \
            --port 8200 \
            --disable-log-requests \
            --disable-uvicorn-access-log \
            --enable-expert-parallel \
            --data-parallel-hybrid-lb \
            --tensor-parallel-size $TP_SIZE \
            --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
            --data-parallel-size-local $DP_SIZE_LOCAL \
            --data-parallel-address ${LWS_LEADER_ADDRESS} \
            --data-parallel-rpc-port 5555 \
            --data-parallel-start-rank $START_RANK \
            --trust-remote-code \
            --enable-eplb \
            --num-redundant-experts 104 \
            --eplb-window-size 1000 \
            --eplb-step-interval 6000 \
            --eplb-log-balancedness

      ports:
        - containerPort: 8200
          name: metrics
          protocol: TCP
      env:
        - name: VLLM_MOE_DP_CHUNK_SIZE
          value: "512"
        - name: DP_SIZE_LOCAL
          value: "8"
        - name: TRITON_LIBCUDA_PATH
          value: "/usr/lib64"
        - name: HF_HUB_DISABLE_XET
          value: "1"
        - name: VLLM_SKIP_P2P_CHECK
          value: "1"
        - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
          value: "1"
        - name: VLLM_USE_DEEP_GEMM
          value: "1"
        - name: VLLM_ALL2ALL_BACKEND
          value: "deepep_low_latency"
        - name: NVIDIA_GDRCOPY
          value: "enabled"
        - name: NVSHMEM_DEBUG
          value: "INFO"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibgda"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: VLLM_LOGGING_LEVEL
          value: "INFO"
        - name: HF_HUB_CACHE
          value: /huggingface-cache/hub
        - name: HF_HUB_OFFLINE
          value: "1"
        - name: VLLM_TORCH_PROFILER_DIR
          value: /home/vllm
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
      resources:
        limits:
          cpu: 32
          memory: 256Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: 8
        requests:
          cpu: 32
          memory: 256Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: 8
      mountModelVolume: false
      volumeMounts:
        - name: dshm
          mountPath: /dev/shm
        - name: hf-cache
          mountPath: /huggingface-cache
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 8Gi
    - name: hf-cache
      hostPath:
        path: /home/smo/.cache/huggingface
        type: Directory
prefill: 
  priorityClassName: moe-testing
  create: true
  replicas: 4
  parallelism:
    # TODO: The value for parallelism.data is a hack to get the pod-per-node case working.
    # This must equal the number of nodes rather than the dp_size.
    data: 1
    tensor: 1 # these will be derived based performance testing
  monitoring:
    podmonitor:
      enabled: false
      portName: "metrics" # decode vLLM service port (from routing.proxy.targetPort)
      path: "/metrics"
      interval: "30s"
  containers:
    - name: vllm-worker-decode
      image: "rgshaw2/vllm-torch-28-0.2"
      imagePullPolicy: Always
      workingDir: /code
      command: ["/bin/bash", "-c"]
      modelCommand: "custom"
      args:
        - |-
          #################
          # RUN vLLM decode worker
          #################
          START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))

          source /opt/vllm/bin/activate
          exec vllm serve \
            deepseek-ai/DeepSeek-V3-0324 \
            --port 8200 \
            --disable-log-requests \
            --disable-uvicorn-access-log \
            --enable-expert-parallel \
            --data-parallel-hybrid-lb \
            --tensor-parallel-size $TP_SIZE \
            --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
            --data-parallel-size-local $DP_SIZE_LOCAL \
            --data-parallel-address ${LWS_LEADER_ADDRESS} \
            --data-parallel-rpc-port 5555 \
            --data-parallel-start-rank $START_RANK \
            --trust-remote-code
      ports:
        - containerPort: 8200
          name: metrics
          protocol: TCP
      env:
        - name: VLLM_MOE_DP_CHUNK_SIZE
          value: "1024"
        - name: DP_SIZE_LOCAL
          value: "8"
        - name: TRITON_LIBCUDA_PATH
          value: "/usr/lib64"
        - name: HF_HUB_DISABLE_XET
          value: "1"
        - name: VLLM_SKIP_P2P_CHECK
          value: "1"
        - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
          value: "1"
        - name: VLLM_USE_DEEP_GEMM
          value: "1"
        - name: VLLM_ALL2ALL_BACKEND
          value: "deepep_high_throughput"
        - name: NVIDIA_GDRCOPY
          value: "enabled"
        - name: NVSHMEM_DEBUG
          value: "INFO"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibgda"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: VLLM_LOGGING_LEVEL
          value: "INFO"
        - name: HF_HUB_CACHE
          value: /huggingface-cache/hub
        - name: HF_HUB_OFFLINE
          value: "1"
        - name: VLLM_TORCH_PROFILER_DIR
          value: /home/vllm
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
      resources:
        limits:
          cpu: 32
          memory: 256Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: 8
        requests:
          cpu: 32
          memory: 256Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: 8
      mountModelVolume: false
      volumeMounts:
        - name: dshm
          mountPath: /dev/shm
        - name: hf-cache
          mountPath: /huggingface-cache
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 8Gi
    - name: hf-cache
      hostPath:
        path: /home/smo/.cache/huggingface
        type: Directory
