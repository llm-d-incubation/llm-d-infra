apiVersion: apps/v1
kind: Deployment
metadata:
  name: ms-inference-scheduling-llm-d-modelservice-decode # @Gregory-Pereira maybe can rename
spec:
  replicas: 2
  selector:
    matchLabels: # @Gregory-Pereira, these were refactored, make sure to test this works for selectors
      app: "llm-d.ai"
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: "meta-llama/Llama-3.1-70B"
      llm-d.ai/role: "decode"
  template:
    metadata:
      labels:  # @Gregory-Pereira, these were refactored, make sure to test this works for selectors
        app: "llm-d.ai"
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: "meta-llama/Llama-3.1-70B"
        llm-d.ai/role: "decode"
    spec:
      initContainers:
        - name: routing-proxy
          args:
            - --port=8000
            - --vllm-port=8200
            - --connector=nixlv2
            - -v=5
            - --secure-proxy=false
          image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.2.0
          imagePullPolicy: Always
          ports:
            - containerPort: 8000
          resources: {}
          restartPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true    
      nodeSelector:
        cloud.google.com/gke-tpu-accelerator: tpu-v6e-slice
        cloud.google.com/gke-tpu-topology: 2x4
      serviceAccountName: ms-inference-scheduling-llm-d-modelservice # @Gregory-Pereira can maybe rename this
      volumes:
        - emptyDir: {}
          name: metrics-volume
        - emptyDir: {}
          name: torch-compile-cache
        - name: model-storage
          emptyDir:
            sizeLimit: 160Gi  
      containers:
        - name: vllm
          image: vllm/vllm-tpu:e92694b6fe264a85371317295bca6643508034ef
          command: ["vllm", "serve"]
          args:
            - meta-llama/Llama-3.1-70B
            - --port
            - "8200"
            - --served-model-name
            - "meta-llama/Llama-3.1-70B"
            - --tensor-parallel-size=8
            - --max-model-len=4096
          env:
          - name: DP_SIZE # @Gregory-Pereira might not need to set this
            value: "1"
          - name: TP_SIZE # @Gregory-Pereira might not need to set this
            value: "1"
          - name: HF_HOME
            value: /model-cache
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: llm-d-hf-token
                key: HF_TOKEN
          ports:
          - containerPort: 5557
            protocol: TCP
          - containerPort: 8200
            name: metrics
            protocol: TCP
          resources:
            limits:
              google.com/tpu: 8
              nvidia.com/gpu: "1" # @liu-cong do we need both tpu + GPU or is this a mistake
            requests:
              google.com/tpu: 8
              nvidia.com/gpu: "1" # @liu-cong do we need both tpu + GPU or is this a mistake
          volumeMounts:
            - mountPath: /.config
              name: metrics-volume
            - mountPath: /.cache
              name: torch-compile-cache
            - name: model-storage
              mountPath: /model-cache