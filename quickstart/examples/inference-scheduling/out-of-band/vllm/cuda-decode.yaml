apiVersion: apps/v1
kind: Deployment
metadata:
  name: ms-inference-scheduling-llm-d-modelservice-decode # @Gregory-Pereira maybe can rename
spec:
  replicas: 2
  selector:
    matchLabels: # @Gregory-Pereira, these were refactored, make sure to test this works for selectors
      app: "llm-d.ai"
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: "Qwen/Qwen3-0.6B"
      llm-d.ai/role: "decode"
  template:
    metadata:
      labels:   # @Gregory-Pereira, these were refactored, make sure to test this works for selectors
        app: "llm-d.ai"
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: "Qwen/Qwen3-0.6B"
        llm-d.ai/role: "decode"
    spec:
      initContainers:
        - name: routing-proxy
          args:
            - --port=8000
            - --vllm-port=8200
            - --connector=nixlv2
            - -v=5
            - --secure-proxy=false
          image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.2.0
          imagePullPolicy: Always
          ports:
            - containerPort: 8000
          resources: {}
          restartPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
      serviceAccountName: ms-inference-scheduling-llm-d-modelservice # @Gregory-Pereira can maybe rename this
      volumes:
        - emptyDir: {}
          name: metrics-volume
        - emptyDir: {}
          name: torch-compile-cache
        - name: model-storage
          emptyDir:
            sizeLimit: 20Gi
      containers:
        - name: vllm
          image: ghcr.io/llm-d/llm-d-dev:pr-170
          command: ["vllm", "serve"]
          args:
            - Qwen/Qwen3-0.6B
            - --port
            - "8200"
            - --served-model-name
            - "Qwen/Qwen3-0.6B"
            - --enforce-eager
            - --kv-transfer-config
            - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
          env:
          - name: CUDA_VISIBLE_DEVICES 
            value: "0" # Do we need to manually expose all CUDA devices?
          - name: UCX_TLS
            value: cuda_ipc,cuda_copy,tcp
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: VLLM_NIXL_SIDE_CHANNEL_PORT
            value: "5557"
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          - name: DP_SIZE # @Gregory-Pereira might not need to set this
            value: "1"
          - name: TP_SIZE # @Gregory-Pereira might not need to set this
            value: "1"
          - name: HF_HOME
            value: /model-cache
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: llm-d-hf-token
                key: HF_TOKEN
          ports:
          - containerPort: 5557
            protocol: TCP
          - containerPort: 8200
            name: metrics
            protocol: TCP
          resources:
            limits:
              nvidia.com/gpu: "1"
            requests:
              nvidia.com/gpu: "1"
          volumeMounts:
            - mountPath: /.config
              name: metrics-volume
            - mountPath: /.cache
              name: torch-compile-cache
            - name: model-storage
              mountPath: /model-cache