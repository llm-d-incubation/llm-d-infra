multinode: true

modelArtifacts:
  uri: "hf://Qwen/Qwen3-30B-A3B-FP8"
  size: 100Gi
  authSecretName: "llm-d-hf-token"

routing:
  modelName: Qwen/Qwen3-30B-A3B-FP8
  servicePort: 8000
  parentRefs:
    - group: gateway.networking.k8s.io
      kind: Gateway
      name: infra-dp-inference-gateway

  proxy:
    image: "ghcr.io/llm-d/llm-d-routing-sidecar:v0.2.0-RC1"
    secure: false
    # Specify a connector to overwrite the default: `nixl`
    connector: nixlv2

  inferenceModel:
    create: false

  inferencePool:
    create: false
    name: gaie-dp

  httpRoute:
    create: true

  epp:
    create: false

decode:
  subGroupPolicy:
    subGroupSize: 2
  create: true
  replicas: 1
  acceleratorTypes:
    labelKey: gpu.nvidia.com/model
    labelValues:
      - H200
  parallelism:
    data: 2 # these will be derived based performance testing
    tensor: 1 # these will be derived based performance testing
  containers:
  - name: vllm-worker-decode
    image: "quay.io/wseaton/vllm:llmd-multistage-6"
    imagePullPolicy: Always
    workingDir: /code
    command: ["/bin/bash","-c"]
    modelCommand: "custom"
    args:
      - |
        # Decode worker
        source /opt/vllm/bin/activate
        exec vllm serve \
          Qwen/Qwen3-30B-A3B-FP8 \
          --port 8200 \
          --disable-log-requests \
          --disable-uvicorn-access-log \
          --enable-expert-parallel \
          --tensor-parallel-size $TP_SIZE \
          --data-parallel-size ${LWS_GROUP_SIZE} \
          --data-parallel-address ${LWS_LEADER_ADDRESS} \
          --data-parallel-rpc-port 5555 \
          --data-parallel-rank ${LWS_WORKER_INDEX} \
          --trust-remote-code \
          --enforce-eager \
          --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
    env:
      - name: UCX_TLS
        value: "^cuda_ipc"
      - name: HF_HUB_DISABLE_XET
        value: "1"
      - name: VLLM_SKIP_P2P_CHECK
        value: "1"
      - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
        value: "1"
      # - name: TP_SIZE
      #   value: "1"
      # - name: VLLM_USE_DEEP_GEMM #turn off if issues
      #   value: "1"
      - name: VLLM_ALL2ALL_BACKEND
        value: "pplx"
      # - name: VLLM_ALL2ALL_BACKEND
      #   value: "deepep_low_latency" # TEMP workaround to avoid issues with nvshmem
      - name: NVIDIA_GDRCOPY
        value: "enabled"
      - name: NVSHMEM_DEBUG
        value: "INFO"
      - name: NVSHMEM_REMOTE_TRANSPORT
        value: "ibgda"
      - name: NVSHMEM_IB_ENABLE_IBGDA
        value: "true"
      - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
        value: "eth0"
      - name: GLOO_SOCKET_IFNAME
        value: "eth0"
      - name: NCCL_SOCKET_IFNAME
        value: "eth0"
      - name: NCCL_IB_HCA
        value: "ibp"
      - name: VLLM_LOGGING_LEVEL
        value: "DEBUG"
      # - name: HF_HUB_CACHE
      #   value: /huggingface-cache
      # - name: HF_TOKEN
      #   valueFrom:
      #     secretKeyRef:
      #       name: llm-d-hf-token
      #       key: HF_TOKEN
      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
    resources:
      limits:
        memory: 128Gi
        ephemeral-storage: 64Gi
        nvidia.com/gpu: "2"
        rdma/ib: 1
      requests:
        cpu: 8
        memory: 128Gi
        ephemeral-storage: 64Gi
        nvidia.com/gpu: "2"
        rdma/ib: 1
    mountModelVolume: false
  #     volumeMounts:
  #       - name: hf-cache
  #         mountPath: /huggingface-cache
  # volumes:
  #   - name: hf-cache
  #     hostPath:
  #       path: /mnt/local/hf-cache
  #       type: DirectoryOrCreate

prefill:
  subGroupPolicy:
    subGroupSize: 2
  create: true
  replicas: 1
  acceleratorTypes:
    labelKey: gpu.nvidia.com/model
    labelValues:
      - H200
  parallelism:
    data: 2 # these will be derived based performance testing
    tensor: 1 # these will be derived based performance testing
  containers:
  - name: vllm-worker-prefill
    image: "quay.io/wseaton/vllm:llmd-multistage-6"
    imagePullPolicy: Always
    workingDir: /code
    command: ["/bin/bash","-c"]
    modelCommand: "custom"
    args:
      - |
        # Prefill worker
        source /opt/vllm/bin/activate
        exec vllm serve \
          Qwen/Qwen3-30B-A3B-FP8 \
          --port 8000 \
          --disable-log-requests \
          --disable-uvicorn-access-log \
          --enable-expert-parallel \
          --tensor-parallel-size $TP_SIZE \
          --data-parallel-size ${LWS_GROUP_SIZE} \
          --data-parallel-address ${LWS_LEADER_ADDRESS} \
          --data-parallel-rpc-port 5555 \
          --data-parallel-rank ${LWS_WORKER_INDEX} \
          --trust-remote-code \
          --enforce-eager \
          --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
    env:
      - name: UCX_TLS
        value: "^cuda_ipc"
      - name: HF_HUB_DISABLE_XET
        value: "1"
      - name: VLLM_SKIP_P2P_CHECK
        value: "1"
      - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
        value: "1"
      # - name: TP_SIZE
      #   value: "1"
      # - name: VLLM_USE_DEEP_GEMM #turn off if issues
      #   value: "1"
      - name: VLLM_ALL2ALL_BACKEND
        value: "pplx"
      # - name: VLLM_ALL2ALL_BACKEND
      #   value: "deepep_high_throughput" # TEMP workaround to avoid issues with nvshmem
      - name: NVIDIA_GDRCOPY
        value: "enabled"
      - name: NVSHMEM_DEBUG
        value: "INFO"
      - name: NVSHMEM_REMOTE_TRANSPORT
        value: "ibgda"
      - name: NVSHMEM_IB_ENABLE_IBGDA
        value: "true"
      - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
        value: "eth0"
      - name: GLOO_SOCKET_IFNAME
        value: "eth0"
      - name: NCCL_SOCKET_IFNAME
        value: "eth0"
      - name: NCCL_IB_HCA
        value: "ibp"
      - name: VLLM_LOGGING_LEVEL
        value: "DEBUG"
      # - name: HF_HUB_CACHE
      #   value: /huggingface-cache
      # - name: HF_TOKEN
      #   valueFrom:
      #     secretKeyRef:
      #       name: llm-d-hf-token
      #       key: HF_TOKEN
      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
    resources:
      limits:
        memory: 128Gi
        ephemeral-storage: 64Gi
        nvidia.com/gpu: "2"
        rdma/ib: 1
      requests:
        cpu: 8
        memory: 128Gi
        ephemeral-storage: 64Gi
        nvidia.com/gpu: "2"
        rdma/ib: 1
    mountModelVolume: false
  #     volumeMounts:
  #       - name: hf-cache
  #         mountPath: /huggingface-cache
  # volumes:
  #   - name: hf-cache
  #     hostPath:
  #       path: /mnt/local/hf-cache
  #       type: DirectoryOrCreate
