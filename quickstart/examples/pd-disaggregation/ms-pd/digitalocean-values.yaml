# DigitalOcean Minimal P/D Disaggregation Override
# This file contains ONLY the values that need to be overridden for DigitalOcean
# Architecture: 1 Prefill Pod (1 GPU) + 1 Decode Pod (1 GPU) = 2 GPUs total

# Use smaller model optimized for DigitalOcean GPU nodes
modelArtifacts:
  uri: "hf://meta-llama/Llama-3.2-3B-Instruct"
  size: 30Gi
  name: "meta-llama/Llama-3.2-3B-Instruct"

routing:
  modelName: meta-llama/Llama-3.2-3B-Instruct

# Decode Pod: Override to single GPU setup optimized for DigitalOcean
decode:
  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d:v0.2.0"
    modelCommand: vllmServe
    args:
      - "--tensor-parallel-size"
      - "1"  # Single GPU instead of 4
      - "--block-size"
      - "128"
      - "--kv-transfer-config"
      - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
      - "--disable-log-requests"
      - "--disable-uvicorn-access-log"
      - "--max-model-len"
      - "8192"  # Smaller context for less VRAM
      - "--gpu-memory-utilization"
      - "0.85"
    env:
      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
    ports:
      - containerPort: 8200
        name: metrics
        protocol: TCP
    resources:
      limits:
        memory: 16Gi
        cpu: "4"
        nvidia.com/gpu: "1"  # Single GPU, no RDMA for DOKS compatibility
      requests:
        memory: 16Gi
        cpu: "4" 
        nvidia.com/gpu: "1"
    mountModelVolume: true
    volumeMounts:
    - name: shm
      mountPath: /dev/shm
  volumes:
  - name: shm
    emptyDir:
      medium: Memory
      sizeLimit: "4Gi"
  # DigitalOcean GPU node tolerations
  tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"

# Prefill Pod: Override to single replica optimized for DigitalOcean
prefill:
  replicas: 1  # Single prefill pod instead of 4
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d:v0.2.0"
    modelCommand: vllmServe
    args:
      - "--tensor-parallel-size"
      - "1"  # Single GPU
      - "--block-size"
      - "128"
      - "--kv-transfer-config"
      - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
      - "--disable-log-requests"
      - "--disable-uvicorn-access-log"
      - "--max-model-len"
      - "8192"
      - "--gpu-memory-utilization"
      - "0.85"
    env:
      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
    ports:
      - containerPort: 8000
        name: metrics
        protocol: TCP
    resources:
      limits:
        memory: 16Gi
        cpu: "4" 
        nvidia.com/gpu: "1"  # Single GPU, no RDMA for DOKS compatibility
      requests:
        memory: 16Gi
        cpu: "4"
        nvidia.com/gpu: "1"
    mountModelVolume: true
    volumeMounts:
    - name: shm
      mountPath: /dev/shm
  volumes:
  - name: shm
    emptyDir:
      medium: Memory
      sizeLimit: "4Gi"
  # DigitalOcean GPU node tolerations
  tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"